# -*- coding: utf-8 -*-
"""streamlit_fmain.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rT1xq-f2GmOowgKzarTRHxkp-1zFmQic
"""

# -*- coding: utf-8 -*-
"""trail 2.1 for website.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1T6gfFxLGjP1Q3-j_t3KUAZeWAVrosF2m
"""

from google.colab import drive
drive.mount('/content/drive')

# Import necessary libraries
import sys
import glob
import json
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
import cv2

import torch
import torch.nn as nn
from torch.optim import Adam
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
from torchvision.models import vgg16, VGG16_Weights
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing import image

# Define constants
IMAGE_SIZE = (224, 224)
BATCH_SIZE = 32
EPOCHS = 10
N_CLASSES = 1
DATA_LABELS = ["best_image", "delete_image"]
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# TensorFlow Section
# Prepare ImageDataGenerator for training and validation
train_datagen = ImageDataGenerator(
    rescale=1.0/255,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    validation_split=0.2
)

train_generator = train_datagen.flow_from_directory(
    '/content/drive/MyDrive/Supervised Dataset',  # Replace with the path to your dataset
    target_size=IMAGE_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='binary',
    subset='training'
)

validation_generator = train_datagen.flow_from_directory(
    '/content/drive/MyDrive/Supervised Dataset',  # Replace with the path to your dataset
    target_size=IMAGE_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='binary',
    subset='validation'
)

# Load ResNet50 model pre-trained on ImageNet
base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze the base model layers
for layer in base_model.layers:
    layer.trainable = False

# Add custom layers on top
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(1024, activation='relu')(x)
predictions = Dense(1, activation='sigmoid')(x)

# Create the model
model = Model(inputs=base_model.input, outputs=predictions)

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(
    train_generator,
    epochs=EPOCHS,
    validation_data=validation_generator
)

# Plot accuracy
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')
plt.show()

# Plot loss
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

# Example usage with a list of image paths
test_image_paths = [
    "/content/drive/MyDrive/Supervised Dataset/Best image/WhatsApp Image 2024-08-03 at 8.06.55 PM.jpeg",
    "/content/drive/MyDrive/Supervised Dataset/Delete Images/WhatsApp Image 2024-08-03 at 8.06.30 PM.jpeg",
    "/content/drive/MyDrive/Supervised Dataset/Delete Images/WhatsApp Image 2024-08-03 at 8.06.33 PM.jpeg"
]

# Define the predict_and_recommend function
def predict_and_recommend(model, image_paths):
  """
  Predicts the class of images and recommends the best and images to delete.

  Args:
      model: The trained machine learning model.
      image_paths: A list of paths to the images.

  Returns:
      A tuple containing:
          - best_image: The path to the best image.
          - delete_images: A list of paths to images recommended for deletion.
  """
  best_image = None
  delete_images = []
  best_score = 0

  for img_path in image_paths:
      img = image.load_img(img_path, target_size=IMAGE_SIZE)
      img = image.img_to_array(img)
      img = np.expand_dims(img, axis=0)
      img /= 255.  # Rescale pixel values

      prediction = model.predict(img)
      score = prediction[0][0]

      if score > best_score:
          best_score = score
          best_image = img_path
      else:
          delete_images.append(img_path)

  return best_image, delete_images

best_image, delete_images = predict_and_recommend(model, test_image_paths)

print("Best Image:")
print(best_image)

print("Images to Delete:")
for img in delete_images:
    print(img)

import matplotlib.pyplot as plt
from PIL import Image

# Function to display images
def display_image(image_path, title):
    image = Image.open(image_path)
    plt.imshow(image)
    plt.title(title)
    plt.axis('off')
    plt.show()

# Display the best image
print("Best Image:")
display_image(best_image, "Best Image")

# Display images to be deleted
print("Images to Delete:")
for img in delete_images:
    display_image(img, "To Be Deleted")

# Unfreeze the base model
vgg_model.requires_grad_(True)
optimizer = Adam(my_model.parameters(), lr=.000001)



# Save the model in .keras format
model.save('/content/model.keras')

from google.colab import files
files.download('/content/model.keras')

